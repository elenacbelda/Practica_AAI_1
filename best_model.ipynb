{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con train_data, debido al tamaño del dataframe de test_data, es necesario dividirlo en lotes para trabajar con ellos individualmente y no cargar la memoria.\n",
    "<br><br> Vamos a usar exactamente el mismo código que usamos para train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el archivo de label completo\n",
    "file_name = f'../data/test_data.csv'\n",
    "final_label = pd.read_csv(file_name, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo test_data_part_1.csv guardado con 500009 filas.\n",
      "Archivo test_data_part_2.csv guardado con 500002 filas.\n",
      "Archivo test_data_part_3.csv guardado con 500011 filas.\n",
      "Archivo test_data_part_4.csv guardado con 500005 filas.\n",
      "Archivo test_data_part_5.csv guardado con 212637 filas.\n"
     ]
    }
   ],
   "source": [
    "# Dividimos el archivo en chunks\n",
    "chunk_size = 500000\n",
    "\n",
    "current_chunk = []\n",
    "current_chunk_size = 0\n",
    "file_index = 1\n",
    "last_customer_id = None\n",
    "\n",
    "# Función para guardar el chunk actual en un CSV\n",
    "def save_chunk(chunk, file_index):\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    df_chunk.to_csv(f'test_data_part_{file_index}.csv', index=False)\n",
    "    print(f'Archivo test_data_part_{file_index}.csv guardado con {len(df_chunk)} filas.')\n",
    "\n",
    "\n",
    "for i, row in final_label.iterrows():\n",
    "    customer_id = row['ID']\n",
    "\n",
    "    # Si hemos alcanzado el tamaño máximo del chunk y el nuevo customer_id es diferente al último en el chunk\n",
    "    if current_chunk_size >= chunk_size and customer_id != last_customer_id:\n",
    "        # Guardar el CSV actual\n",
    "        save_chunk(current_chunk, file_index) \n",
    "        file_index += 1\n",
    "        \n",
    "        # Reiniciar el chunk\n",
    "        current_chunk = []  \n",
    "        current_chunk_size = 0\n",
    "\n",
    "    # Añadir la fila actual al chunk\n",
    "    current_chunk.append(row)\n",
    "    current_chunk_size += 1\n",
    "    last_customer_id = customer_id\n",
    "\n",
    "# Guardar el último chunk si no está vacío\n",
    "if current_chunk:\n",
    "    save_chunk(current_chunk, file_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo test_data_part_1.csv cargado en df_data_part_1\n",
      "Archivo test_data_part_2.csv cargado en df_data_part_2\n",
      "Archivo test_data_part_3.csv cargado en df_data_part_3\n",
      "Archivo test_data_part_4.csv cargado en df_data_part_4\n",
      "Archivo test_data_part_5.csv cargado en df_data_part_5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    file_name = f'test_data_part_{i}.csv'\n",
    "    globals()[f'df_data_part_{i}'] = pd.read_csv(file_name, encoding='ISO-8859-1')\n",
    "    print(f'Archivo {file_name} cargado en df_data_part_{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este apartado consiste en el mismo preprocesado que usamos sobre train_data ya que los datos de test tienen que tener la misma estructura que los datos con los que se entrenó el modelo (Nº de columnas, tipo de datos de columnas... ) o dará un problema al realizar la predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar columnas con valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_percentage(df, percentage):\n",
    "    null_ratio = {}\n",
    "    for col in df.columns:\n",
    "        ratio = df[col].isna().sum() / len(df) * 100\n",
    "        if ratio > percentage:\n",
    "            null_ratio[col] = ratio\n",
    "           \n",
    "    return null_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me quedo solo con las columnas con menos de 50% de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con más del 50% de valores nulos en todos los CSVs: ['Infraction_JVWF', 'Infraction_ZVHJ', 'Base_8318', 'Infraction_SBF', 'Expenditure_KMW', 'Infraction_MZI', 'Infraction_NCB', 'Infraction_IRKE', 'Infraction_CLLY', 'Infraction_ADWZ', 'Infraction_GEL', 'Risk_4561', 'Infraction_WEG', 'Infraction_SVKR', 'Infraction_APIU', 'Infraction_ZTLC', 'Infraction_MAN', 'Infraction_GWL', 'Base_64022', 'Infraction_FUSM', 'Infraction_HPS', 'Base_3958', 'Infraction_WLMI', 'Infraction_QGC', 'Infraction_WWLN', 'Infraction_HPLO', 'Base_8379', 'Infraction_EBA', 'Infraction_ANHZ', 'Risk_5797']\n"
     ]
    }
   ],
   "source": [
    "columns_with_high_nulls = []\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    null_columns = set(null_percentage(data_frame, 50).keys())\n",
    "    \n",
    "    # Si es el primer DataFrame, inicializar el conjunto con sus columnas\n",
    "    if i == 1:\n",
    "        columns_with_high_nulls = null_columns\n",
    "    else:\n",
    "        # Mantener solo las columnas que están en todos los DataFrames\n",
    "        columns_with_high_nulls = columns_with_high_nulls.intersection(null_columns)\n",
    "\n",
    "columns_with_high_nulls = list(columns_with_high_nulls)\n",
    "\n",
    "print(f\"Columnas con más del 50% de valores nulos en todos los CSVs: {columns_with_high_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas con más de 50% de valores nulos en todos los csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    data_frame.drop(columns=columns_with_high_nulls, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar si hay filas que tengan todas las columnas a null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay filas completamente nulas en df_data_part_1.\n",
      "No hay filas completamente nulas en df_data_part_2.\n",
      "No hay filas completamente nulas en df_data_part_3.\n",
      "No hay filas completamente nulas en df_data_part_4.\n",
      "No hay filas completamente nulas en df_data_part_5.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay filas completamente nulas\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    null_rows = data_frame[data_frame.isnull().all(axis=1)]\n",
    "    if not null_rows.empty:\n",
    "        print(f'Filas completamente nulas en df_data_part_{i}:')\n",
    "        print(null_rows)\n",
    "    else:\n",
    "        print(f'No hay filas completamente nulas en df_data_part_{i}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay filas duplicadas en df_data_part_1.\n",
      "No hay filas duplicadas en df_data_part_2.\n",
      "No hay filas duplicadas en df_data_part_3.\n",
      "No hay filas duplicadas en df_data_part_4.\n",
      "No hay filas duplicadas en df_data_part_5.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay filas duplicadas en cada DataFrame\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    duplicate_rows = data_frame[data_frame.duplicated()]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f'Filas duplicadas en df_data_part_{i}:')\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(f'No hay filas duplicadas en df_data_part_{i}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar varianza 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay columnas con baja varianza en df_data_part_1.\n",
      "No hay columnas con baja varianza en df_data_part_2.\n",
      "No hay columnas con baja varianza en df_data_part_3.\n",
      "No hay columnas con baja varianza en df_data_part_4.\n",
      "No hay columnas con baja varianza en df_data_part_5.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay columnas con baja varianza\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    low_variance_cols = []\n",
    "    for col in data_frame.columns:\n",
    "        if data_frame[col].nunique() == 1:\n",
    "            low_variance_cols.append(col)\n",
    "    if low_variance_cols:\n",
    "        print(f'Columnas con baja varianza en df_data_part_{i}: {low_variance_cols}')\n",
    "    else:\n",
    "        print(f'No hay columnas con baja varianza en df_data_part_{i}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de tipo de variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object: ['ID', 'Expenditure_AHF', 'Infraction_YFSG', 'Infraction_DQLY', 'Infraction_CLH', 'Base_67254', 'Infraction_TEN']\n",
      "float64: ['Payment_6804', 'Infraction_CGP', 'Base_7744', 'Base_80863', 'Risk_1930', 'Expenditure_JIG', 'Infraction_SNZ', 'Base_02683', 'Infraction_ZWWJ', 'Infraction_QJJF', 'Base_76065', 'Infraction_EJZ', 'Base_6872', 'Risk_0322', 'Infraction_FMXQ', 'Infraction_GGO', 'Infraction_TLPJ', 'Base_1165', 'Base_39598', 'Base_6187', 'Infraction_ZTNC', 'Base_85131', 'Risk_9995', 'Infraction_AYWV', 'Payment_22507', 'Base_9516', 'Expenditure_YTR', 'Base_36384', 'Expenditure_FIP', 'Infraction_PAS', 'Risk_0003', 'Expenditure_HMO', 'Base_24406', 'Expenditure_LMSR', 'Infraction_BSU', 'Base_14808', 'Risk_8065', 'Infraction_ZYW', 'Base_1039', 'Infraction_HSSU', 'Infraction_EHZP', 'Infraction_TBP', 'Base_0580', 'Expenditure_RGD', 'Infraction_PBC', 'Infraction_AQO', 'Base_0229', 'Base_69608', 'Base_91828', 'Base_6852', 'Expenditure_IDZ', 'Risk_1475', 'Expenditure_BWX', 'Base_8511', 'Infraction_JYZB', 'Base_22178', 'Infraction_ZTYG', 'Infraction_ZVW', 'Infraction_EYU', 'Expenditure_UWVG', 'Base_3041', 'Payment_3207', 'Infraction_QKZN', 'Infraction_CZE', 'Base_65352', 'Risk_7095', 'Infraction_JBR', 'Base_66195', 'Base_36516', 'Infraction_RXQH', 'Infraction_HFU', 'Risk_6346', 'Expenditure_HRQ', 'Infraction_VTR', 'Risk_2102', 'Risk_4804', 'Base_7331', 'Infraction_XWX', 'Expenditure_XDD', 'Risk_4553', 'Base_67585', 'Risk_8742', 'Infraction_VHU', 'Risk_4247', 'Risk_2380', 'Infraction_GSS', 'Risk_0454', 'Base_8730', 'Expenditure_HKXV', 'Infraction_MHM', 'Risk_4160', 'Risk_3506', 'Expenditure_GCAO', 'Risk_9367', 'Base_7910', 'Expenditure_GMC', 'Risk_9423', 'Risk_6977', 'Base_9103', 'Infraction_KSBR', 'Risk_6178', 'Risk_6197', 'Infraction_NRBQ', 'Infraction_WVC', 'Infraction_QVSL', 'Infraction_QXUM', 'Risk_8532', 'Risk_9247', 'Infraction_IMIM', 'Expenditure_UIWS', 'Expenditure_ONEG', 'Expenditure_MTRQ', 'Expenditure_LAHK', 'Expenditure_HPM', 'Infraction_LTIS', 'Infraction_HFSI', 'Infraction_ETH', 'Infraction_SDWM', 'Base_5441', 'Base_2810', 'Risk_8902', 'Infraction_PTY', 'Infraction_BGGU', 'Base_4569', 'Expenditure_BEH', 'Infraction_LMHK', 'Infraction_NMCB', 'Infraction_TPAF', 'Infraction_ZRH', 'Infraction_XEPQ', 'Infraction_ZMKI', 'Infraction_WIS', 'Infraction_RKTA', 'Infraction_IIZ', 'Infraction_WVAW', 'Infraction_KEJT', 'Infraction_TFOY', 'Infraction_WMAQ', 'Infraction_SIA', 'Infraction_CZXL', 'Infraction_QEY', 'Base_52892', 'Infraction_HUK', 'Infraction_VHHP', 'Infraction_LIES', 'Risk_5270', 'Infraction_QWWW', 'Infraction_YQXM', 'Infraction_QGR', 'Infraction_LSX', 'Infraction_IBJ', 'Infraction_DNOU']\n",
      "int64: ['Base_23737']\n"
     ]
    }
   ],
   "source": [
    "# Agrupar columnas por tipo de dato\n",
    "data_types = {}\n",
    "# No hace falta comprobar todos los df ya que tienen las mismas columnas\n",
    "data_frame = df_data_part_1\n",
    "for col in data_frame.columns:\n",
    "    data_type = str(data_frame[col].dtype)\n",
    "    if data_type not in data_types:\n",
    "        data_types[data_type] = []\n",
    "    data_types[data_type].append(col)\n",
    "\n",
    "for data_type, columns in data_types.items():\n",
    "    print(f'{data_type}: {columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación ID to_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos ID a número en todos los dataframes para facilitar su procesado\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    data_frame['ID'] = pd.to_numeric(data_frame['ID'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación Expenditure_AHF to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    data_frame['Expenditure_AHF'] = pd.to_datetime(data_frame['Expenditure_AHF'], errors='coerce')\n",
    "    data_frame['Expenditure_AHF_year'] = data_frame['Expenditure_AHF'].dt.year\n",
    "    data_frame['Expenditure_AHF_month'] = data_frame['Expenditure_AHF'].dt.month\n",
    "    data_frame['Expenditure_AHF_day'] = data_frame['Expenditure_AHF'].dt.day\n",
    "    data_frame.drop(columns=['Expenditure_AHF'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación Infraction_YFSG encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'XL', 'CL', 'XZ', 'CR', 'CO', 'XM'}\n"
     ]
    }
   ],
   "source": [
    "# Ver los valores de la columna Infraction_YFSG en todos los DataFrames y hacer una intersección\n",
    "unique_values = set()\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    unique_values_i = set(data_frame['Infraction_YFSG'].unique())\n",
    "    if not unique_values:\n",
    "        unique_values = unique_values_i\n",
    "    else:\n",
    "        unique_values = unique_values.intersection(unique_values_i)\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    data_frame['Infraction_YFSG_encoded'] = label_encoder.fit_transform(data_frame['Infraction_YFSG'])\n",
    "    data_frame.drop(columns=['Infraction_YFSG'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación Infraction_DQLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-1', nan, 'U', 'R', 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Ver los valores de la columna Infraction_YFSG en todos los DataFrames y hacer una intersección\n",
    "unique_values = set()\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    unique_values_i = set(data_frame['Infraction_DQLY'].unique())\n",
    "    if not unique_values:\n",
    "        unique_values = unique_values_i\n",
    "    else:\n",
    "        unique_values = unique_values.intersection(unique_values_i)\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de NaNs en df_data_part_1: 3.90%\n",
      "Porcentaje de NaNs en df_data_part_2: 3.85%\n",
      "Porcentaje de NaNs en df_data_part_3: 4.00%\n",
      "Porcentaje de NaNs en df_data_part_4: 3.92%\n",
      "Porcentaje de NaNs en df_data_part_5: 3.96%\n"
     ]
    }
   ],
   "source": [
    "# Análisis de porcentaje de NaNs en cada DataFrame en la columna Infraction_DQLY\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    print(f'Porcentaje de NaNs en df_data_part_{i}: {data_frame[\"Infraction_DQLY\"].isna().mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de codificarlo necesitamos tratar los NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eleni\\AppData\\Local\\Temp\\ipykernel_30248\\3431230910.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_frame['Infraction_DQLY'].fillna(most_common_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Como el porcentaje de NaNs es muy bajo (aprox. 4%), rellenamos los NaNs con el valor más común\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    most_common_value = data_frame['Infraction_DQLY'].mode()[0]\n",
    "    data_frame['Infraction_DQLY'].fillna(most_common_value, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hacemos la codificación de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    data_frame['Infraction_DQLY_encoded'] = label_encoder.fit_transform(data_frame['Infraction_DQLY'])\n",
    "    data_frame.drop(columns=['Infraction_DQLY'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las siguientes columnas comparten etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infraction_CLH: ['very_high' nan 'moderate_low' 'moderate' 'high' 'moderate_high' 'low'\n",
      " 'very_low']\n",
      "Base_67254: ['moderate_low' 'low' 'high' 'moderate' 'moderate_high' 'very_high'\n",
      " 'extremely_high' nan]\n",
      "Infraction_TEN: ['moderate_high' 'extremely_low' nan 'moderate_low' 'very_high' 'moderate'\n",
      " 'high' 'low']\n"
     ]
    }
   ],
   "source": [
    "ordinal_columns = ['Infraction_CLH', 'Base_67254', 'Infraction_TEN']\n",
    "\n",
    "for col in ordinal_columns:\n",
    "    print(f'{col}: {df_data_part_1[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map = {\n",
    "    'extremely_low': 0,\n",
    "    'very_low': 1,\n",
    "    'moderate_low': 2,\n",
    "    'low': 3,\n",
    "    'moderate': 4,\n",
    "    'high':5,\n",
    "    'moderate_high': 6,\n",
    "    'very_high': 7,\n",
    "    'extremely_high': 8\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(1, 6):\n",
    "    for col in ordinal_columns:\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        data_frame[f'{col}_encoded'] = data_frame[col].map(frequency_map)\n",
    "        data_frame.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infraction_CLH_encoded: [ 7. nan  2.  4.  5.  6.  3.  1.]\n",
      "Base_67254_encoded: [ 2.  3.  5.  4.  6.  7.  8. nan]\n",
      "Infraction_TEN_encoded: [ 6.  0. nan  2.  7.  4.  5.  3.]\n"
     ]
    }
   ],
   "source": [
    "ordinal_columns = ['Infraction_CLH_encoded', 'Base_67254_encoded', 'Infraction_TEN_encoded']\n",
    "\n",
    "for col in ordinal_columns:\n",
    "    print(f'{col}: {df_data_part_1[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infraction_CLH_encoded: 3.91%\n",
      "Base_67254_encoded: 0.03%\n",
      "Infraction_TEN_encoded: 3.18%\n"
     ]
    }
   ],
   "source": [
    "# Voy a ver cuántos valores NaN hay en cada columna de las ordinal_columns en la intersection de los DataFrames como porcentaje\n",
    "for col in ordinal_columns:\n",
    "    nan_count = 0\n",
    "    total = 0\n",
    "    for i in range(1, 6):\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        total += data_frame[col].shape[0]\n",
    "        nan_count += data_frame[col].isna().sum()\n",
    "    print(f'{col}: {nan_count/total*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un porcentaje muy bajo así que podemos rellenarlos sin problema. En este caso usaremos la moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infraction_CLH_encoded: [np.float64(7.0), np.float64(7.0), np.float64(7.0), np.float64(7.0), np.float64(7.0)]\n",
      "Base_67254_encoded: [np.float64(2.0), np.float64(2.0), np.float64(2.0), np.float64(2.0), np.float64(2.0)]\n",
      "Infraction_TEN_encoded: [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Quiero saber el valor que más se repite en cada columna ordinal\n",
    "for col in ordinal_columns:\n",
    "    most_common_values = []\n",
    "    for i in range(1, 6):\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        most_common_value = data_frame[col].mode()[0]\n",
    "        most_common_values.append(most_common_value)\n",
    "    print(f'{col}: {most_common_values}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN con el valor más común en cada columna ordinal.\n",
    "for col in ordinal_columns:\n",
    "    for i in range(1, 6):\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        most_common_value = data_frame[col].mode()[0]\n",
    "        data_frame[col] = data_frame[col].fillna(most_common_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay columnas duplicadas en df_data_part_1.\n",
      "No hay columnas duplicadas en df_data_part_2.\n",
      "No hay columnas duplicadas en df_data_part_3.\n",
      "No hay columnas duplicadas en df_data_part_4.\n",
      "No hay columnas duplicadas en df_data_part_5.\n"
     ]
    }
   ],
   "source": [
    "# Ya hemos visto que las categóricas no están dulpicadas entre sí así que solo analizamos si las columnas numéricas están duplicadas\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    duplicate_columns = data_frame.columns[data_frame.columns.duplicated()]\n",
    "    if not duplicate_columns.empty:\n",
    "        print(f'Columnas duplicadas en df_data_part_{i}: {duplicate_columns}')\n",
    "    else:\n",
    "        print(f'No hay columnas duplicadas en df_data_part_{i}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar las filas para eliminar aquellas que tengan NaN en más del 30% de las columnas si estas componen una pequeña poción del dataframe y rellenar estos valores no tendría sentido ya que la fila perdería valor para el posterior estudio ya que sería completamente artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas a eliminar: 0.26%\n"
     ]
    }
   ],
   "source": [
    "# Quiero saber cuántas filas tienen más del 30% de las columnas nulas en porcentaje sobre el total de todos los DataFrames\n",
    "rows_to_drop = []\n",
    "total_rows = 0\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    total_rows += data_frame.shape[0]\n",
    "    null_percentage = data_frame.isnull().mean(axis=1) * 100\n",
    "    rows_to_drop.extend(null_percentage[null_percentage > 30].index)\n",
    "rows_to_drop = set(rows_to_drop)\n",
    "print(f'Número de filas a eliminar: {len(rows_to_drop)/total_rows*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminos las filas con más del 30% de valores nulos\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    # Como no me deja usar los valores de rows_to_drop como índices, lo hago de la siguiente manera\n",
    "    data_frame.drop(data_frame.index.intersection(rows_to_drop), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer un análisis de las columnas que nos quedan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payment_6804: 0.57987%\n",
      "Base_80863: 0.00380%\n",
      "Expenditure_JIG: 18.46015%\n",
      "Infraction_SNZ: 0.00380%\n",
      "Base_02683: 0.00380%\n",
      "Infraction_ZWWJ: 29.71958%\n",
      "Infraction_QJJF: 4.70174%\n",
      "Infraction_EJZ: 0.00384%\n",
      "Infraction_FMXQ: 21.72539%\n",
      "Infraction_TLPJ: 12.83084%\n",
      "Base_1165: 0.00018%\n",
      "Base_6187: 0.37463%\n",
      "Infraction_AYWV: 0.28399%\n",
      "Payment_22507: 5.17378%\n",
      "Infraction_PAS: 0.00380%\n",
      "Expenditure_HMO: 18.46015%\n",
      "Infraction_BSU: 3.13765%\n",
      "Base_14808: 0.74555%\n",
      "Infraction_HSSU: 1.68441%\n",
      "Infraction_TBP: 10.75337%\n",
      "Base_0580: 0.05204%\n",
      "Infraction_PBC: 13.60488%\n",
      "Base_0229: 0.00380%\n",
      "Base_91828: 0.00380%\n",
      "Base_6852: 0.00380%\n",
      "Infraction_JYZB: 3.26058%\n",
      "Base_22178: 0.00380%\n",
      "Infraction_ZTYG: 1.44665%\n",
      "Infraction_EYU: 0.18233%\n",
      "Infraction_QKZN: 0.27311%\n",
      "Infraction_JBR: 45.37687%\n",
      "Base_66195: 0.05204%\n",
      "Base_36516: 0.00380%\n",
      "Infraction_RXQH: 4.70174%\n",
      "Infraction_HFU: 1.10742%\n",
      "Infraction_VTR: 0.27311%\n",
      "Base_7331: 0.00380%\n",
      "Infraction_XWX: 0.23186%\n",
      "Risk_4553: 0.00146%\n",
      "Infraction_VHU: 3.26058%\n",
      "Infraction_GSS: 0.28399%\n",
      "Base_8730: 0.00380%\n",
      "Risk_9423: 0.00160%\n",
      "Base_9103: 0.00380%\n",
      "Infraction_KSBR: 0.28399%\n",
      "Infraction_NRBQ: 2.61581%\n",
      "Expenditure_UIWS: 0.33754%\n",
      "Expenditure_ONEG: 0.00814%\n",
      "Expenditure_MTRQ: 0.32931%\n",
      "Expenditure_LAHK: 0.23026%\n",
      "Expenditure_HPM: 0.01134%\n",
      "Infraction_LTIS: 0.53171%\n",
      "Infraction_HFSI: 1.57296%\n",
      "Infraction_ETH: 1.57296%\n",
      "Infraction_SDWM: 1.57296%\n",
      "Base_2810: 0.00087%\n",
      "Risk_8902: 2.20849%\n",
      "Infraction_PTY: 0.01171%\n",
      "Infraction_BGGU: 0.01514%\n",
      "Base_4569: 0.00082%\n",
      "Expenditure_BEH: 25.34829%\n",
      "Infraction_LMHK: 2.92230%\n",
      "Infraction_NMCB: 2.92230%\n",
      "Infraction_TPAF: 2.92230%\n",
      "Infraction_ZRH: 2.92230%\n",
      "Infraction_XEPQ: 2.92230%\n",
      "Infraction_ZMKI: 2.92230%\n",
      "Infraction_WIS: 2.92230%\n",
      "Infraction_RKTA: 2.92230%\n",
      "Infraction_IIZ: 2.92230%\n",
      "Infraction_WVAW: 2.92230%\n",
      "Infraction_KEJT: 2.92230%\n",
      "Infraction_TFOY: 2.92230%\n",
      "Infraction_WMAQ: 1.89651%\n",
      "Infraction_CZXL: 1.57296%\n",
      "Infraction_QEY: 1.57296%\n",
      "Base_52892: 0.01280%\n",
      "Infraction_HUK: 1.57296%\n",
      "Infraction_VHHP: 1.57296%\n",
      "Infraction_LIES: 0.57040%\n",
      "Infraction_QWWW: 1.57296%\n",
      "Infraction_YQXM: 0.53135%\n",
      "Infraction_QGR: 1.57296%\n",
      "Infraction_LSX: 1.57296%\n",
      "Infraction_IBJ: 0.53396%\n",
      "Infraction_DNOU: 1.57296%\n"
     ]
    }
   ],
   "source": [
    "# Voy a ver cuántos valores NaN hay en cada columna en la intersection de los DataFrames.\n",
    "columns_percentage = {}\n",
    "for col in df_data_part_1.columns:\n",
    "    nan_count = 0\n",
    "    total_rows = 0\n",
    "    for i in range(1, 6):\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        total_rows += data_frame.shape[0]\n",
    "        nan_count += data_frame[col].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        columns_percentage[col] = nan_count/total_rows * 100    \n",
    "        print(f'{col}: {nan_count/total_rows * 100:.5f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a rellenar aquellas que tengan menos del 6% de NaNs y a eliminar los que tengan más de ese porcentaje , rellenar estos últimos no tendría sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas con más de 6% de valores nulos en la intersección de los DataFrames\n",
    "for col in columns_percentage.keys():\n",
    "    if columns_percentage[col] > 6:\n",
    "        for i in range(1, 6):\n",
    "            data_frame = globals()[f'df_data_part_{i}']\n",
    "            data_frame.drop(columns=[col], inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a rellenar los NaNs de las columnas restantes de la siguiente manera:\n",
    "# Si la columna es categórica, rellena con la moda (no hace falta tenerlo en cuenta ahora ya que lo hemos hecho manualmnete anteriormente)\n",
    "# Si la columna es numérica:\n",
    "#           - con la media si la desviación estándar es menor a 1\n",
    "#           - con la mediana si la desviación estándar es mayor a 1\n",
    "#           - con la moda si la desviación estándar es 0\n",
    "\n",
    "for col in df_data_part_1.columns:\n",
    "    for i in range(1, 6):\n",
    "        data_frame = globals()[f'df_data_part_{i}']\n",
    "        if data_frame[col].isna().sum() != 0:\n",
    "            mean = data_frame[col].mean()\n",
    "            median = data_frame[col].median()\n",
    "            mode = data_frame[col].mode()[0]\n",
    "            std = data_frame[col].std()\n",
    "            if std == 0:\n",
    "                data_frame[col] = data_frame[col].fillna(mode)\n",
    "            elif std < 1:\n",
    "                data_frame[col] = data_frame[col].fillna(mean)\n",
    "            else:\n",
    "                data_frame[col] = data_frame[col].fillna(median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar valores infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores infinitos en df_data_part_1: 0.0\n",
      "Valores infinitos en df_data_part_2: 0.0\n",
      "Valores infinitos en df_data_part_3: 0.0\n",
      "Valores infinitos en df_data_part_4: 0.0\n",
      "Valores infinitos en df_data_part_5: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los valores infinitos para tratarlos\n",
    "from numpy import inf\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    infinite_values = data_frame[data_frame == np.inf].sum().sum()\n",
    "    print(f'Valores infinitos en df_data_part_{i}: {infinite_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "    \n",
    "    X = data_frame  \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    globals()[f'df_scaled_part_{i}'] = X_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baja varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas a eliminar por baja varianza en todas las particiones: {'Risk_6197', 'Risk_5270', 'Infraction_KSBR', 'Risk_8742', 'Base_23737', 'Base_5441', 'Infraction_ZRH', 'Base_7331', 'Risk_9247', 'Risk_3506', 'Infraction_PTY', 'Risk_6178', 'Expenditure_GCAO', 'Risk_4160'}\n"
     ]
    }
   ],
   "source": [
    "# Voy a hacer una selección de características a eliminar con el método de la varianza (threshold=0.01)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "columnas_baja_varianza_final = None\n",
    "\n",
    "for i in range(1, 6):\n",
    "    data_frame = globals()[f'df_data_part_{i}']\n",
    "\n",
    "    X = data_frame\n",
    "\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    selector.fit(X)\n",
    "    columnas_baja_varianza = X.columns[~selector.get_support()].tolist()\n",
    "    if i == 1:\n",
    "        columnas_baja_varianza_final = set(columnas_baja_varianza)\n",
    "    else:\n",
    "        columnas_baja_varianza_final.intersection_update(columnas_baja_varianza)\n",
    "\n",
    "print(f'Columnas a eliminar por baja varianza en todas las particiones: {columnas_baja_varianza_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    data_frame = globals()[f'df_scaled_part_{i}']\n",
    "    data_frame.drop(columns=columnas_baja_varianza_final, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de características "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(pd.read_pickle('selected_features_lists/selected_features_CI.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    data_frame = globals()[f'df_scaled_part_{i}']\n",
    "    globals()[f'df_final_test_{i}'] =  data_frame[['ID'] + selected_features ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de dataframes preprocesados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    globals()[f'df_final_test_{i}'].to_csv(f'test_data_part_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejor modelo: red de neuronas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo, cargamos los datos preprocesados de train y ajustamos los hiperparámetros de la red de neuronal para conseguir los mejores resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargado de train dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_1: Archivo df_train_1 cargado\n",
      "TRAIN_2: Archivo df_train_2 cargado\n",
      "TRAIN_3: Archivo df_train_3 cargado\n",
      "TRAIN_4: Archivo df_train_4 cargado\n",
      "TRAIN_5: Archivo df_train_5 cargado\n",
      "TRAIN_6: Archivo df_train_6 cargado\n",
      "TRAIN_7: Archivo df_train_7 cargado\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 8):\n",
    "    file_name_train = f'../data/dataFrame_final/final_train/df_final_train_{i}.csv'\n",
    "    globals()[f'df_train_{i}'] = pd.read_csv(file_name_train, encoding='ISO-8859-1')\n",
    "    print(f'TRAIN_{i}: Archivo df_train_{i} cargado')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(255, activation='relu'),\n",
    "        Dropout( 0.31666966256912804),\n",
    "        Dense(124, activation='relu'),\n",
    "        Dropout(0.3008937986275955),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = globals()['df_train_1'].shape[1] - 1  \n",
    "model = create_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=3,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for i in range(1, 8):  \n",
    "        data_train = globals()[f'df_train_{i}']\n",
    "        X_train = data_train.drop(columns=['label']).values\n",
    "        y_train = data_train['label'].values\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción con el modelo ajustado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"../data/models/Neural_network.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo test_data_part_1.csv cargado\n",
      "Archivo test_data_part_2.csv cargado\n",
      "Archivo test_data_part_3.csv cargado\n",
      "Archivo test_data_part_4.csv cargado\n",
      "Archivo test_data_part_5.csv cargado\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    file_name_train = f'../test_data/test_data_part_{i}.csv'\n",
    "    globals()[f'df_data_part_{i}'] = pd.read_csv(file_name_train, encoding='ISO-8859-1')\n",
    "    print(f'Archivo test_data_part_{i}.csv cargado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15445/15445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 560us/step\n",
      "\u001b[1m15444/15444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 469us/step\n",
      "\u001b[1m15445/15445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 484us/step\n",
      "\u001b[1m15444/15444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 482us/step\n",
      "\u001b[1m6559/6559\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 490us/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    data = globals()[f'df_data_part_{i}']\n",
    "    X = data.values\n",
    "    raw_predictions = model.predict(X)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    globals()[f'df_data_part_{i}']['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de clientes: 183459\n"
     ]
    }
   ],
   "source": [
    "total_clients = set()  \n",
    "\n",
    "for i in range(1, 6):\n",
    "    data = globals()[f'df_data_part_{i}']\n",
    "    total_clients.update(data['ID'].unique())  \n",
    "\n",
    "print(f'Número total de clientes: {len(total_clients)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataframe test_labels debe de contener una sola fila por cliente asociado a una columna 'label' que determina si el cliente es fraudulento o no. Ahora sabemos que todo el dataframe de test contiene la información de 183.459 clientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a fusionar las predicciones para label de cada transacción de un único cliente siguiendo la siguiente lógica: \n",
    "- Si una sola transacción de un cliente se considera fraudulenta, label será 1\n",
    "- Si todas las transacciones son 0, label será 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_label = pd.DataFrame()\n",
    "\n",
    "for i in range(1, 6):\n",
    "    \n",
    "    data = globals()[f'df_data_part_{i}']\n",
    "    \n",
    "    \n",
    "    data = data[['ID', 'label']]\n",
    "    \n",
    "    # Usamos max() para que si alguna transacción tiene 'label' 1, se asigne 1 al cliente\n",
    "    data_grouped = data.groupby('ID', as_index=False).agg({'label': 'max'})\n",
    "    \n",
    "    final_label = pd.concat([final_label, data_grouped], ignore_index=True)\n",
    "\n",
    "# Agrupamos nuevamente por 'ID' en caso de que haya clientes duplicados entre partes y aplicamos la misma lógica\n",
    "# final_label = final_label.groupby('ID', as_index=False).agg({'label': 'max'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183459, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label.to_csv('test_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
